{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Posture Recognition Dataset Exploration\n",
        "\n",
        "This notebook explores the posture dataset and provides insights for model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append('../src')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "from collections import Counter\n",
        "\n",
        "# Import our preprocessing functions\n",
        "from preprocess import prepare_data, visualize_samples\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "DATA_DIR = \"../data\"\n",
        "X_train, X_val, y_train, y_val, class_names = prepare_data(data_dir=DATA_DIR)\n",
        "\n",
        "print(f\"Dataset Summary:\")\n",
        "print(f\"- Total training samples: {len(X_train)}\")\n",
        "print(f\"- Total validation samples: {len(X_val)}\")\n",
        "print(f\"- Image shape: {X_train[0].shape}\")\n",
        "print(f\"- Number of classes: {len(class_names)}\")\n",
        "print(f\"- Class names: {class_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Class Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze class distribution\n",
        "train_labels = np.argmax(y_train, axis=1)\n",
        "val_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "train_counts = Counter(train_labels)\n",
        "val_counts = Counter(val_labels)\n",
        "\n",
        "# Create visualization\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Training set distribution\n",
        "classes = [class_names[i] for i in range(len(class_names))]\n",
        "train_values = [train_counts[i] for i in range(len(class_names))]\n",
        "ax1.bar(classes, train_values, alpha=0.7)\n",
        "ax1.set_title('Training Set Class Distribution')\n",
        "ax1.set_ylabel('Number of Samples')\n",
        "for i, v in enumerate(train_values):\n",
        "    ax1.text(i, v + max(train_values)*0.01, str(v), ha='center')\n",
        "\n",
        "# Validation set distribution\n",
        "val_values = [val_counts[i] for i in range(len(class_names))]\n",
        "ax2.bar(classes, val_values, alpha=0.7, color='orange')\n",
        "ax2.set_title('Validation Set Class Distribution')\n",
        "ax2.set_ylabel('Number of Samples')\n",
        "for i, v in enumerate(val_values):\n",
        "    ax2.text(i, v + max(val_values)*0.01, str(v), ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Check for class imbalance\n",
        "total_train = sum(train_values)\n",
        "for i, (class_name, count) in enumerate(zip(classes, train_values)):\n",
        "    percentage = (count / total_train) * 100\n",
        "    print(f\"{class_name}: {count} samples ({percentage:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sample Images Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images from each class\n",
        "print(\"Sample images from the dataset:\")\n",
        "visualize_samples(X_train, y_train, class_names, num_samples=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Image Statistics Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze pixel value distributions\n",
        "def analyze_image_statistics(images, labels, class_names):\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # Overall pixel distribution\n",
        "    all_pixels = images.flatten()\n",
        "    axes[0, 0].hist(all_pixels, bins=50, alpha=0.7)\n",
        "    axes[0, 0].set_title('Overall Pixel Value Distribution')\n",
        "    axes[0, 0].set_xlabel('Pixel Value')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    \n",
        "    # RGB channel distributions\n",
        "    colors = ['red', 'green', 'blue']\n",
        "    for i, color in enumerate(colors):\n",
        "        channel_pixels = images[:, :, :, i].flatten()\n",
        "        axes[0, 1].hist(channel_pixels, bins=50, alpha=0.5, label=f'{color.title()} channel', color=color)\n",
        "    axes[0, 1].set_title('RGB Channel Distributions')\n",
        "    axes[0, 1].set_xlabel('Pixel Value')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].legend()\n",
        "    \n",
        "    # Mean pixel values per class\n",
        "    class_means = []\n",
        "    class_stds = []\n",
        "    \n",
        "    for class_idx in range(len(class_names)):\n",
        "        class_mask = np.argmax(labels, axis=1) == class_idx\n",
        "        class_images = images[class_mask]\n",
        "        \n",
        "        if len(class_images) > 0:\n",
        "            mean_val = np.mean(class_images)\n",
        "            std_val = np.std(class_images)\n",
        "            class_means.append(mean_val)\n",
        "            class_stds.append(std_val)\n",
        "        else:\n",
        "            class_means.append(0)\n",
        "            class_stds.append(0)\n",
        "    \n",
        "    axes[0, 2].bar(class_names, class_means, yerr=class_stds, alpha=0.7)\n",
        "    axes[0, 2].set_title('Mean Pixel Values by Class')\n",
        "    axes[0, 2].set_ylabel('Mean Pixel Value')\n",
        "    \n",
        "    # Brightness distribution per class\n",
        "    for class_idx in range(len(class_names)):\n",
        "        class_mask = np.argmax(labels, axis=1) == class_idx\n",
        "        class_images = images[class_mask]\n",
        "        \n",
        "        if len(class_images) > 0:\n",
        "            # Calculate brightness (mean of RGB values)\n",
        "            brightness = np.mean(class_images, axis=(1, 2, 3))\n",
        "            axes[1, class_idx].hist(brightness, bins=30, alpha=0.7)\n",
        "            axes[1, class_idx].set_title(f'Brightness Distribution - {class_names[class_idx]}')\n",
        "            axes[1, class_idx].set_xlabel('Brightness')\n",
        "            axes[1, class_idx].set_ylabel('Frequency')\n",
        "    \n",
        "    # If we have less than 3 classes, hide the extra subplot\n",
        "    if len(class_names) < 3:\n",
        "        axes[1, 2].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(\"\\nDataset Statistics:\")\n",
        "    print(f\"Overall mean pixel value: {np.mean(all_pixels):.3f}\")\n",
        "    print(f\"Overall std pixel value: {np.std(all_pixels):.3f}\")\n",
        "    print(f\"Min pixel value: {np.min(all_pixels):.3f}\")\n",
        "    print(f\"Max pixel value: {np.max(all_pixels):.3f}\")\n",
        "\n",
        "analyze_image_statistics(X_train, y_train, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Augmentation Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create augmentation generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Show original vs augmented images\n",
        "def show_augmentation_examples(image, num_examples=4):\n",
        "    fig, axes = plt.subplots(1, num_examples + 1, figsize=(15, 3))\n",
        "    \n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Original')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # Augmented images\n",
        "    image_batch = np.expand_dims(image, axis=0)\n",
        "    augmented_iter = datagen.flow(image_batch, batch_size=1)\n",
        "    \n",
        "    for i in range(num_examples):\n",
        "        augmented_image = next(augmented_iter)[0]\n",
        "        # Ensure values are in [0, 1] range for display\n",
        "        augmented_image = np.clip(augmented_image, 0, 1)\n",
        "        \n",
        "        axes[i + 1].imshow(augmented_image)\n",
        "        axes[i + 1].set_title(f'Augmented {i + 1}')\n",
        "        axes[i + 1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Show augmentation examples for different classes\n",
        "for class_idx in range(len(class_names)):\n",
        "    class_mask = np.argmax(y_train, axis=1) == class_idx\n",
        "    class_images = X_train[class_mask]\n",
        "    \n",
        "    if len(class_images) > 0:\n",
        "        print(f\"\\nAugmentation examples for {class_names[class_idx]} class:\")\n",
        "        show_augmentation_examples(class_images[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dataset Quality Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assess_dataset_quality(images, labels, class_names):\n",
        "    print(\"Dataset Quality Assessment:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # 1. Check for data leakage (identical images)\n",
        "    print(\"\\n1. Checking for duplicate images...\")\n",
        "    image_hashes = []\n",
        "    for img in images:\n",
        "        # Simple hash based on mean values\n",
        "        img_hash = hash(tuple(img.flatten()[:100]))  # Use first 100 pixels for speed\n",
        "        image_hashes.append(img_hash)\n",
        "    \n",
        "    unique_hashes = len(set(image_hashes))\n",
        "    total_images = len(image_hashes)\n",
        "    duplicate_count = total_images - unique_hashes\n",
        "    \n",
        "    print(f\"   - Total images: {total_images}\")\n",
        "    print(f\"   - Unique images: {unique_hashes}\")\n",
        "    print(f\"   - Potential duplicates: {duplicate_count}\")\n",
        "    if duplicate_count > 0:\n",
        "        print(\"   ‚ö†Ô∏è  Warning: Potential duplicate images detected\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ No duplicate images detected\")\n",
        "    \n",
        "    # 2. Check image quality metrics\n",
        "    print(\"\\n2. Image quality metrics...\")\n",
        "    \n",
        "    # Calculate contrast and sharpness\n",
        "    contrasts = []\n",
        "    sharpness_scores = []\n",
        "    \n",
        "    for img in images[:100]:  # Sample first 100 for speed\n",
        "        # Convert to grayscale for analysis\n",
        "        gray = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "        \n",
        "        # Contrast (standard deviation of pixel values)\n",
        "        contrast = np.std(gray)\n",
        "        contrasts.append(contrast)\n",
        "        \n",
        "        # Sharpness (variance of Laplacian)\n",
        "        laplacian_var = cv2.Laplacian((gray * 255).astype(np.uint8), cv2.CV_64F).var()\n",
        "        sharpness_scores.append(laplacian_var)\n",
        "    \n",
        "    print(f\"   - Average contrast: {np.mean(contrasts):.3f} ¬± {np.std(contrasts):.3f}\")\n",
        "    print(f\"   - Average sharpness: {np.mean(sharpness_scores):.1f} ¬± {np.std(sharpness_scores):.1f}\")\n",
        "    \n",
        "    # Quality thresholds (these are rough guidelines)\n",
        "    low_contrast_threshold = 0.05\n",
        "    low_sharpness_threshold = 100\n",
        "    \n",
        "    low_contrast_count = sum(1 for c in contrasts if c < low_contrast_threshold)\n",
        "    low_sharpness_count = sum(1 for s in sharpness_scores if s < low_sharpness_threshold)\n",
        "    \n",
        "    if low_contrast_count > len(contrasts) * 0.1:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: {low_contrast_count} images have low contrast\")\n",
        "    if low_sharpness_count > len(sharpness_scores) * 0.1:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: {low_sharpness_count} images may be blurry\")\n",
        "    \n",
        "    # 3. Class balance analysis\n",
        "    print(\"\\n3. Class balance analysis...\")\n",
        "    label_counts = []\n",
        "    for class_idx in range(len(class_names)):\n",
        "        count = np.sum(np.argmax(labels, axis=1) == class_idx)\n",
        "        label_counts.append(count)\n",
        "    \n",
        "    min_count = min(label_counts)\n",
        "    max_count = max(label_counts)\n",
        "    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
        "    \n",
        "    print(f\"   - Imbalance ratio: {imbalance_ratio:.2f}\")\n",
        "    if imbalance_ratio > 3:\n",
        "        print(\"   ‚ö†Ô∏è  Warning: Significant class imbalance detected\")\n",
        "        print(\"   üí° Consider using class weights or resampling techniques\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ Classes are reasonably balanced\")\n",
        "    \n",
        "    # 4. Recommendations\n",
        "    print(\"\\n4. Recommendations:\")\n",
        "    if total_images < 1000:\n",
        "        print(\"   üìà Consider collecting more data for better performance\")\n",
        "    if duplicate_count > 0:\n",
        "        print(\"   üîç Remove duplicate images to prevent overfitting\")\n",
        "    if imbalance_ratio > 3:\n",
        "        print(\"   ‚öñÔ∏è  Address class imbalance with weighted loss or data augmentation\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "\n",
        "assess_dataset_quality(X_train, y_train, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature correlations between classes\n",
        "def analyze_class_separability(images, labels, class_names):\n",
        "    print(\"Class Separability Analysis:\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Extract simple features for each image\n",
        "    features = []\n",
        "    for img in images:\n",
        "        # Calculate various statistical features\n",
        "        gray = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "        \n",
        "        feature_vector = [\n",
        "            np.mean(gray),           # Average brightness\n",
        "            np.std(gray),            # Contrast\n",
        "            np.mean(img[:, :, 0]),   # Red channel mean\n",
        "            np.mean(img[:, :, 1]),   # Green channel mean\n",
        "            np.mean(img[:, :, 2]),   # Blue channel mean\n",
        "            np.std(img[:, :, 0]),    # Red channel std\n",
        "            np.std(img[:, :, 1]),    # Green channel std\n",
        "            np.std(img[:, :, 2]),    # Blue channel std\n",
        "        ]\n",
        "        features.append(feature_vector)\n",
        "    \n",
        "    features = np.array(features)\n",
        "    feature_names = ['Brightness', 'Contrast', 'Red_mean', 'Green_mean', 'Blue_mean',\n",
        "                    'Red_std', 'Green_std', 'Blue_std']\n",
        "    \n",
        "    # Create box plots for each feature by class\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        data_by_class = []\n",
        "        for class_idx in range(len(class_names)):\n",
        "            class_mask = np.argmax(labels, axis=1) == class_idx\n",
        "            class_features = features[class_mask, i]\n",
        "            data_by_class.append(class_features)\n",
        "        \n",
        "        axes[i].boxplot(data_by_class, labels=class_names)\n",
        "        axes[i].set_title(feature_name)\n",
        "        axes[i].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Calculate feature importance (simple difference between class means)\n",
        "    print(\"\\nFeature separability scores:\")\n",
        "    for i, feature_name in enumerate(feature_names):\n",
        "        class_means = []\n",
        "        for class_idx in range(len(class_names)):\n",
        "            class_mask = np.argmax(labels, axis=1) == class_idx\n",
        "            class_mean = np.mean(features[class_mask, i])\n",
        "            class_means.append(class_mean)\n",
        "        \n",
        "        # Calculate separability as difference between class means\n",
        "        separability = abs(class_means[0] - class_means[1]) if len(class_means) >= 2 else 0\n",
        "        print(f\"   {feature_name}: {separability:.4f}\")\n",
        "\n",
        "analyze_class_separability(X_train, y_train, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_training_recommendations(X_train, X_val, y_train, y_val, class_names):\n",
        "    print(\"Training Recommendations:\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    total_samples = len(X_train) + len(X_val)\n",
        "    train_ratio = len(X_train) / total_samples\n",
        "    \n",
        "    print(f\"\\nüìä Dataset Size: {total_samples} total samples\")\n",
        "    print(f\"   - Training: {len(X_train)} ({train_ratio:.1%})\")\n",
        "    print(f\"   - Validation: {len(X_val)} ({1-train_ratio:.1%})\")\n",
        "    \n",
        "    # Batch size recommendations\n",
        "    print(f\"\\n‚öôÔ∏è  Recommended Training Parameters:\")\n",
        "    \n",
        "    if total_samples < 500:\n",
        "        print(f\"   - Batch size: 16-32 (small dataset)\")\n",
        "        print(f\"   - Epochs: 50-100 (more epochs for small data)\")\n",
        "        print(f\"   - Learning rate: 0.001-0.01\")\n",
        "    elif total_samples < 2000:\n",
        "        print(f\"   - Batch size: 32-64\")\n",
        "        print(f\"   - Epochs: 30-50\")\n",
        "        print(f\"   - Learning rate: 0.001\")\n",
        "    else:\n",
        "        print(f\"   - Batch size: 64-128\")\n",
        "        print(f\"   - Epochs: 20-30\")\n",
        "        print(f\"   - Learning rate: 0.001\")\n",
        "    \n",
        "    # Data augmentation recommendations\n",
        "    print(f\"\\nüîÑ Data Augmentation:\")\n",
        "    if total_samples < 1000:\n",
        "        print(f\"   - Use aggressive augmentation\")\n",
        "        print(f\"   - Rotation: ¬±20¬∞, shifts: ¬±15%, zoom: ¬±15%\")\n",
        "    else:\n",
        "        print(f\"   - Use moderate augmentation\")\n",
        "        print(f\"   - Rotation: ¬±15¬∞, shifts: ¬±10%, zoom: ¬±10%\")\n",
        "    \n",
        "    # Architecture recommendations\n",
        "    print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
        "    if total_samples < 500:\n",
        "        print(f\"   - Use smaller model (MobileNetV2 with fewer layers)\")\n",
        "        print(f\"   - Add more dropout (0.5-0.7)\")\n",
        "        print(f\"   - Consider transfer learning with frozen layers\")\n",
        "    else:\n",
        "        print(f\"   - MobileNetV2 or EfficientNet-B0 recommended\")\n",
        "        print(f\"   - Standard dropout (0.3-0.5)\")\n",
        "        print(f\"   - Fine-tuning after initial training\")\n",
        "    \n",
        "    # Monitoring recommendations\n",
        "    print(f\"\\nüìà Training Monitoring:\")\n",
        "    print(f\"   - Use early stopping (patience: 10-15)\")\n",
        "    print(f\"   - Monitor validation accuracy\")\n",
        "    print(f\"   - Save best model checkpoints\")\n",
        "    print(f\"   - Use learning rate reduction on plateau\")\n",
        "    \n",
        "    # Expected performance\n",
        "    print(f\"\\nüéØ Expected Performance:\")\n",
        "    if total_samples < 500:\n",
        "        print(f\"   - Target accuracy: 70-85%\")\n",
        "        print(f\"   - Risk of overfitting: High\")\n",
        "    elif total_samples < 2000:\n",
        "        print(f\"   - Target accuracy: 80-90%\")\n",
        "        print(f\"   - Risk of overfitting: Medium\")\n",
        "    else:\n",
        "        print(f\"   - Target accuracy: 85-95%\")\n",
        "        print(f\"   - Risk of overfitting: Low\")\n",
        "    \n",
        "    print(f\"\\n\" + \"=\" * 50)\n",
        "\n",
        "generate_training_recommendations(X_train, X_val, y_train, y_val, class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Dataset Exploration Complete!\")\n",
        "print(\"=\" * 40)\n",
        "print(\"\\n‚úÖ Analysis completed:\")\n",
        "print(\"   - Dataset overview and statistics\")\n",
        "print(\"   - Class distribution analysis\")\n",
        "print(\"   - Image quality assessment\")\n",
        "print(\"   - Data augmentation preview\")\n",
        "print(\"   - Feature separability analysis\")\n",
        "print(\"   - Training recommendations\")\n",
        "\n",
        "print(\"\\nüöÄ Next steps:\")\n",
        "print(\"   1. Review the recommendations above\")\n",
        "print(\"   2. Adjust training parameters in train.py if needed\")\n",
        "print(\"   3. Run the training script: python ../src/train.py\")\n",
        "print(\"   4. Monitor training progress and metrics\")\n",
        "print(\"   5. Test the model with inference.py\")\n",
        "\n",
        "print(\"\\nüìù Notes:\")\n",
        "print(\"   - This notebook analyzed your current dataset\")\n",
        "print(\"   - Recommendations are based on dataset size and quality\")\n",
        "print(\"   - Monitor validation metrics during training\")\n",
        "print(\"   - Adjust hyperparameters based on initial results\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}